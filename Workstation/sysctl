## [Improved default settings for Linux machines](https://tobert.github.io/post/2014-06-24-linux-defaults.html)
## [How to Configure nginx for Optimized Performance](https://www.linode.com/docs/websites/nginx/configure-nginx-for-optimized-performance)

# IPv6 disabled
net.ipv6.conf.all.disable_ipv6              = 1
net.ipv6.conf.default.disable_ipv6          = 1
net.ipv6.conf.lo.disable_ipv6               = 1

# Tell the kernel to only swap if it really needs it
vm.swappiness                               = 10

# Increase the number of allowed mmapped files
vm.max_map_count                            = 1048576

# Increase the number of file handles available globally
fs.file-max                                 = 1048576

# Allow up to 999999 processes with corresponding pids
# this looks nice and rarely rolls over - I've never had a problem with it.
kernel.pid_max                              = 999999

# Seconds to delay after a kernel panic and before rebooting automatically
kernel.panic                                = 300

# Do not enable if your machines are not physically secured
# this can be used to force reboots, kill processses, cause kernel crashes, etc without logging in
# but it's very handy when a machine is hung and you need to get control
# that said, I always enable it
kernel.sysrq                                = 1

# The TCP FIN timeout belays the amount of time a port must be inactive before it can reused for another connection.
# The default is often 60 seconds, but can normally be safely reduced to 30 or even 15 seconds:
net.ipv4.tcp_fin_timeout                    = 15

# Updating the net.core.somaxconn and net.ipv4.tcp_max_tw_buckets changes the size of the queue for connections waiting
# for acceptance by nginx. If there are error messages in the kernel log, increase the value until errors stop.
net.core.somaxconn                          = 65536
net.ipv4.tcp_max_tw_buckets                 = 1440000

# Packets can be buffered in the network card before being handed to the CPU by setting the max backlog with the
# net.core.netdev_max_backlog tag.

# This TCP option, along with several others, is defined in IETF RFC 1323 which deals with long fat networks.
# It can be defined with the net.ipv4.tcp_window_scaling = 1 tag.
net.ipv4.tcp_window_scaling                 = 1

# The net.ipv4.tcp_max_syn_backlog determines a number of packets to keep in the backlog before the kernel starts
# dropping them. A sane value is net.ipv4.tcp_max_syn_backlog = 3240000
net.ipv4.tcp_max_syn_backlog                = 3240000

# The IPv4 local port range defines a port range value.
net.ipv4.ip_local_port_range                = 1024 65000

# I recommend Googling "Linux C10k" to learn more about what they mean.
net.ipv4.tcp_rmem                           = 4096 87380 16777216
net.ipv4.tcp_wmem                           = 4096 65536 16777216
net.core.rmem_max                           = 16777216
net.core.wmem_max                           = 16777216

# these will need local tuning, currently set to start flushing dirty pages at 256MB
# writes will start blocking at 2GB of dirty data, but this should only ever happen if
# your disks are far slower than your software is writing data
# If you have an older kernel, you will need to s/bytes/ratio/ and adjust accordingly.
vm.dirty_background_bytes                   = 268435456
vm.dirty_bytes                              = 1073741824

# increase the sysv ipc limits
kernel.shmmax                               = 33554432
kernel.msgmax                               = 33554432
kernel.msgmnb                               = 33554432
